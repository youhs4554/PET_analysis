{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossay/anaconda3/envs/torch/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tqdm\n",
    "from natsort import natsorted\n",
    "import os, sys, random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import skimage\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import resize\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prepro = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "def save_patches(frame_root, save_root, patch_size, global_ix,\n",
    "                 seed, n_sample=100):\n",
    "    if not os.path.exists(save_root):\n",
    "        os.system(f'mkdir -p {save_root}')\n",
    "    \n",
    "    filenames = natsorted(os.listdir(frame_root))\n",
    "\n",
    "    volume = []\n",
    "    for fn in filenames:\n",
    "        fn = os.path.join(frame_root, fn)\n",
    "        #img = skimage.io.imread(fn, plugin='tifffile')\n",
    "        img = image_prepro(Image.open(fn)).numpy()[0]\n",
    "        volume.append(img)\n",
    "\n",
    "    volume = np.array(volume)\n",
    "    \n",
    "    # random volume cropping for 100 times\n",
    "    random.seed(seed)\n",
    "    c = 0\n",
    "    while c < n_sample:\n",
    "        rand_z, rand_y, rand_x = [ np.random.randint(0, volume.shape[i]-patch_size) for i in range(3) ]\n",
    "        crop = volume[rand_z:rand_z+patch_size, rand_y:rand_y+patch_size, rand_x:rand_x+patch_size]\n",
    "        if all([ len(np.unique(s)) > 0.2*(patch_size**2) for s in crop]):\n",
    "            np.save(os.path.join(save_root, f'patch_{global_ix}.npy'), crop)\n",
    "            global_ix += 1\n",
    "            c += 1\n",
    "    \n",
    "    return global_ix\n",
    "    \n",
    "def generate_patches(root, save_root,\n",
    "                    patch_size=7, train=True):\n",
    "    dirpath = np.array(natsorted(glob.glob(root+'/*/*')))\n",
    "\n",
    "    train_size = int(len(dirpath)*0.8)\n",
    "    test_size = len(dirpath)-train_size\n",
    "\n",
    "    np.random.seed(999)\n",
    "    train_ixs = np.random.choice(range(len(dirpath)), size=train_size, replace=False)\n",
    "    test_ixs = np.array(\n",
    "        list(set(range(len(dirpath))) - set(train_ixs))\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        dirpath = dirpath[train_ixs]\n",
    "    else:\n",
    "        dirpath = dirpath[test_ixs]\n",
    "                \n",
    "    suffix = 'train' if train else 'test'\n",
    "                \n",
    "    save_root = os.path.join(save_root, suffix)\n",
    "    \n",
    "    global_ix = 0\n",
    "    for frame_root in tqdm.tqdm(dirpath):\n",
    "        seed = random.randint(-sys.maxsize, sys.maxsize)\n",
    "        global_ix = save_patches(frame_root, save_root, patch_size, global_ix=global_ix, seed=seed, n_sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Patch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/DSMC_breast468/full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/data/DSMC_breast468/patches'):\n",
    "    generate_patches(root, save_root='/data/DSMC_breast468/patches', train=True)  # trainset\n",
    "    generate_patches(root, save_root='/data/DSMC_breast468/patches', train=False) # testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PETImagePatchDataset(Dataset):\n",
    "    def __init__(self, root, patch_size=7, train=True):\n",
    "        suffix = 'train' if train else 'test'\n",
    "        root = os.path.join(root, suffix)\n",
    "        \n",
    "        self.dirpath = np.array(natsorted(glob.glob(root+'/*')))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.dirpath)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        volume = np.load(self.dirpath[ix])\n",
    "        return torch.FloatTensor(volume).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/DSMC_breast468/patches'\n",
    "datasets = {\n",
    "    split : PETImagePatchDataset(root, train=(split=='train'))\n",
    "    for split in ['train', 'test']\n",
    "}\n",
    "dataloaders = {\n",
    "    split: DataLoader(datasets[split], batch_size=128, shuffle=(split=='train'))\n",
    "    for split in ['train', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, datetime\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from networks import SparseAutoencoderKL\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "def kl_divergence(p, p_hat):\n",
    "    funcs = nn.Sigmoid()\n",
    "    p_hat = torch.mean(funcs(p_hat), 1)\n",
    "    p_tensor = torch.Tensor([p] * len(p_hat)).to(device)\n",
    "    return torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
    "\n",
    "def sparse_loss(autoencoder, images):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    for i in range(3):\n",
    "        fc_layer = list(autoencoder.encoder.children())[2 * i]\n",
    "        relu = list(autoencoder.encoder.children())[2 * i + 1]\n",
    "        values = fc_layer(values)\n",
    "        loss += kl_divergence(DISTRIBUTION_VAL, values)\n",
    "    for i in range(2):\n",
    "        fc_layer = list(autoencoder.decoder.children())[2 * i]\n",
    "        relu = list(autoencoder.decoder.children())[2 * i + 1]\n",
    "        values = fc_layer(values)\n",
    "        loss += kl_divergence(DISTRIBUTION_VAL, values)\n",
    "    return loss\n",
    "\n",
    "def model_training(autoencoder, train_loader, epoch):\n",
    "    loss_metric = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    autoencoder.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images = data\n",
    "        images = Variable(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        if cuda: images = images.to(device)\n",
    "        outputs = autoencoder(images)\n",
    "        mse_loss = loss_metric(outputs, images)\n",
    "        kl_loss = sparse_loss(autoencoder, images)\n",
    "        loss = mse_loss + kl_loss * SPARSE_REG\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            print('Epoch [{}/{}] - Iter[{}/{}], Total loss:{:.4f}, MSE loss:{:.4f}, Sparse loss:{:.4f}'.format(\n",
    "                epoch + 1, EPOCHS, i + 1, len(train_loader.dataset) // BATCH_SIZE, loss.item(), mse_loss.item(), kl_loss.item()\n",
    "            ))\n",
    "\n",
    "def evaluation(autoencoder, test_loader):\n",
    "    total_loss = 0\n",
    "    loss_metric = nn.MSELoss()\n",
    "    autoencoder.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images = data\n",
    "        images = Variable(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        if cuda: images = images.to(device)\n",
    "        outputs = autoencoder(images)\n",
    "        loss = loss_metric(outputs, images)\n",
    "        total_loss += loss * len(images)\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nAverage MSE Loss on Test set: {:.4f}'.format(avg_loss))\n",
    "\n",
    "    global BEST_VAL\n",
    "    if TRAIN_SCRATCH and avg_loss < BEST_VAL:\n",
    "        BEST_VAL = avg_loss\n",
    "        torch.save(autoencoder.state_dict(), './history/sparse_autoencoder_KL.pt')\n",
    "        print('Save Best Model in HISTORY\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "LOG_INTERVAL = 100\n",
    "DISTRIBUTION_VAL = 0.3\n",
    "SPARSE_REG = 1e-3\n",
    "TRAIN_SCRATCH = False        # whether to train a model from scratch\n",
    "BEST_VAL = float('inf')     # record the best val loss\n",
    "\n",
    "#train_loader, test_loader = data_utils.load_mnist(BATCH_SIZE)\n",
    "train_loader, test_loader = dataloaders['train'], dataloaders['test']\n",
    "\n",
    "autoencoder = SparseAutoencoderKL()\n",
    "if cuda: autoencoder.to(device)\n",
    "\n",
    "if TRAIN_SCRATCH:\n",
    "    # Training autoencoder from scratch\n",
    "    for epoch in range(EPOCHS):\n",
    "        starttime = datetime.datetime.now()\n",
    "        model_training(autoencoder, train_loader, epoch)\n",
    "        endtime = datetime.datetime.now()\n",
    "        print(f'Train a epoch in {(endtime - starttime).seconds} seconds')\n",
    "        # evaluate on test set and save best model\n",
    "        evaluation(autoencoder, test_loader)\n",
    "    print('Trainig Complete with best validation loss {:.4f}'.format(BEST_VAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average MSE Loss on Test set: 0.0059\n"
     ]
    }
   ],
   "source": [
    "autoencoder.load_state_dict(torch.load('./history/sparse_autoencoder_KL.pt'))\n",
    "evaluation(autoencoder, test_loader)\n",
    "\n",
    "autoencoder.cpu()\n",
    "dataiter = iter(test_loader)\n",
    "images = next(dataiter)\n",
    "images = Variable(images[:32])\n",
    "outputs = autoencoder(images.view(images.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e7254b438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# plot and save original and reconstruction images for comparisons\n",
    "input_sample = images[12].reshape(7,1,7,7)\n",
    "input_sample = (input_sample - input_sample.min())/(input_sample.max()-input_sample.min())\n",
    "input_grid = torchvision.utils.make_grid(input_sample)\n",
    "input_grid = input_grid.cpu().detach().numpy().transpose(1,2,0)\n",
    "\n",
    "output_sample = outputs[12].reshape(7,1,7,7)\n",
    "output_sample = (output_sample - output_sample.min())/(output_sample.max()-output_sample.min())\n",
    "output_grid = torchvision.utils.make_grid(output_sample)\n",
    "output_grid = output_grid.cpu().detach().numpy().transpose(1,2,0)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "merged = np.row_stack((input_grid, output_grid))\n",
    "plt.imshow(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
